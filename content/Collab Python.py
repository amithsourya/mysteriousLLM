# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ys8WI-QTbrzvbut856ouzpaBNPTHdVQ2
"""

!pip install -q transformers  peft accelerate bitsandbytes

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "microsoft/phi-2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)

inputs = tokenizer("Who is Sherlock Holmes?", return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    target_modules=["q_prpj", "v_proj"],
    bias = "none"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from datasets import load_dataset

data = load_dataset("json", data_files="/content/entity_instructions.json")

print(data)

def format_example(example):
  return f"Instruction: {example['instruction']}\nInput: {example['input']}\nOutput: {example['output']}"

print(format_example(data["train"][1]))

def tokenize_function(example):
  return tokenizer(format_example(example), padding="max_length", truncation=True, max_length=512)

if tokenizer.pad_token is None:
  tokenizer.pad_token = tokenizer.eos_token

tokenized_data = data.map(tokenize_function, batched=False)

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

tokenized_data = tokenized_data["train"].train_test_split(test_size=0.05)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=".entity_lora",
    per_device_train_batch_size=2,
    num_train_epochs = 3,
    learning_rate = 2e-4,
    logging_dir = ".logs",
    logging_steps = 10,
    save_steps = 200,
    fp16 = torch.cuda.is_available(),
    report_to = "none",
    save_total_limit = 1
)

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_data["train"],
    eval_dataset = tokenized_data["test"],
    data_collator = data_collator
)

trainer.train()

model.save_pretrained("./entity_lora_adapter")
tokenizer.save_pretrained("./entity_lora_adapter")

from transformers import pipeline

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map = "auto")

pipe("Instruction: Who are you? \nInput: \nOutput:", max_new_tokens=300, do_sample=True, temperature=0.9, top_p=0.95)

pipe("Instruction: Tell me a creepy thing? \nInput: \nOutput:", max_new_tokens=300, do_sample=True, temperature=0.9, top_p=0.95)

pipe("Instruction: Are you alive? \nInput: \nOutput:", max_new_tokens=300, do_sample=True, temperature=0.9, top_p=0.95)

!zip -r /content/file.zip /content

from google.colab import files
files.download("/content/file.zip")

